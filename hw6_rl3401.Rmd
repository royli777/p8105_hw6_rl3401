---
title: "p8105_hw6_rl3401"
author: "Ruoxi Li"
date: "`r Sys.Date()`"
output: github_document
---

## Problem 1
```{r}
library(tidyverse)
library(broom)
library(purrr)
library(ggplot2)
```

Create a city_state variable and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; Kansas City, MO and omit Tulsa, AL. Limit the analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.

```{r}
homicide_df_raw = read.csv("data/homicide-data.csv")

homicide_df = homicide_df_raw |>
  mutate(city_state = paste(city, state, sep = ", "),
         solved = disposition == "Closed by arrest") |>
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
         victim_race %in% c("White", "Black")) |>
  mutate(victim_age = ifelse(victim_age == "Unknown", NA, victim_age)) |>
  mutate(victim_age = as.numeric(victim_age))
```

For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
baltimore_df = homicide_df |>
  filter(city_state == "Baltimore, MD")

baltimore_model = glm(solved ~ victim_age + victim_sex + victim_race, 
                       data = baltimore_df, family = "binomial")

tidy_baltimore_model = tidy(baltimore_model)

gender_comparison = tidy_baltimore_model |>
  filter(term == "victim_sexMale")|>
  mutate(or = exp(estimate),
         lower_ci = exp(estimate - 1.96 * std.error),
         upper_ci = exp(estimate + 1.96 * std.error))|>
  knitr::kable()
```

Run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 

```{r}
city_models = homicide_df |>
  group_by(city_state) |>
  nest() |>
  mutate(model = map(data, ~glm(solved ~ victim_age + victim_sex + victim_race,
                                data = .x, family = "binomial"))) |>
  mutate(tidy_model = map(model, broom::tidy))

or_ci = city_models |>
  mutate(gender_comparison = map(tidy_model, ~.x |>
                                 filter(term == "victim_sexMale") |>
                                 mutate(or = exp(estimate),
                              lower_ci = exp(estimate - 1.96 * std.error),                                   upper_ci = exp(estimate + 1.96 * std.error)))) |>
  select(city_state, gender_comparison) |>
  unnest(gender_comparison)
```

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r}
ggplot(or_ci, aes(x = reorder(city_state, or), y = or)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +
  coord_flip() +
  labs(x = "City", y = "Adjusted Odds Ratio", 
       title = "Adjusted Odds Ratios for Solving Homicides: Male vs Female Victims by City") +
  theme_minimal()
```

## Problem 2

Load the data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

Fit a simple linear regression with `tmax` as the response with `tmin` and `prcp` as the predictors.

```{r}
lm = lm(tmax ~ tmin + prcp, data = weather_df)
   broom::tidy(lm)
   broom::glance(lm)
```

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. 

```{r}
res =   
  weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy),
    results2 = map(models, broom::glance)) |> 
  select(results, results2) |> 
  unnest(results2) |> 
  select(r.squared, results) |>
  unnest(results) |>
  select(term, estimate, r.squared) |>
  group_by(term) |>
  mutate(group_id = ceiling((row_number() ))) %>%
  ungroup() |>
  pivot_wider(
    names_from = term,
    values_from = estimate,
  ) |>
  mutate(log_bata = log ( tmin * abs(prcp)))
```

Identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for $\hat{r}^2$ and $log(\hat{\beta_1} * \hat{\beta_2})$.

```{r}
r_squared_ci <- quantile(res$r.squared, c(0.025, 0.975))

log_beta_ci <- quantile(res$log_beta, c(0.025, 0.975))
```

draw the plot.

```{r}
ggplot(res, aes(x = r.squared)) +
  geom_density()+
  labs(x = "Bootstrap R-squared)", y = "Frequency") +
  ggtitle("Distribution of Bootstrap Estimates for r̂²")
```

The goodness of fit of a model can be measured by $\hat{r}^2$.

The plot is slightly left-skewed, with the range appears to be from just below 0.88 to just below 0.95.

The highest frequency of values is around 0.92, which suggests that 0.92 is the most common estimate.

The curve is smooth, which suggests that the bootstrap samples were large enough to provide a good approximation of the distribution.


```{r}
ggplot(res, aes(x = log_bata)) +
  geom_density()+
  labs(x = "Bootstrap Log(Beta1*Beta2)", y = "Frequency") +
  ggtitle("Distribution of Bootstrap Estimates for  Log(Beta1*Beta2)")
```

The logarithm of absolute values was used because $hat{\beta_2}$ was negative, which cannot be directly logged.

A higher value in this range indicates a stronger influence of the two factors. 

The logarithmic values of the distribution range from approximately -9 to -4, indicating a broad spread in the influence of two factors. Most values center around -5, suggesting a moderate influence. The distribution is left-skewed, meaning there's a higher frequency of higher influence values. 

## Problem 3

Load the data.

```{r}
birthweight_df = read_csv('data/birthweight.csv') 

sum(is.na(birthweight_df))
```

There is no missing data. So next I converted numeric to factor where appropriate.

```{r}
birthweight_df$babysex <- as.factor(birthweight_df$babysex)
birthweight_df$frace <- as.factor(birthweight_df$frace)
birthweight_df$mrace <- as.factor(birthweight_df$mrace)
birthweight_df$malform <- as.factor(birthweight_df$malform)
```

Fit a linear regression model.

```{r}
summary(birthweight_df)

# exclude pnumlbw and pnumsga because all 0

test_fit =
  birthweight_df |>
  lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks + malform + menarche + mheight + momage + mrace + parity + ppbmi + ppwt + smoken + wtgain, data = _) |>
  broom::tidy() |>
  filter(p.value < 0.01)

my_fit = 
  birthweight_df |>
  lm(bwt ~ mrace + blength + bhead+babysex+gaweeks, data = _)
```

I choose `bhead`, `blength`, `babysex`, `gaweeks` and `mrace` as the predictors, because they have the largest absolute coefficient(estimate>10) estimate and a small p value(p<0.01).


```{r}
birthweight_df |>
  modelr::add_residuals(my_fit) |> 
  mutate(
    pred = modelr::add_predictions(birthweight_df, my_fit) |> pull(pred)
  ) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  labs(title = "Residuals vs fitted values")
```

For most of the fitted values, residuals spread randomly around zero. However, the residual distribution is slightly skewed and has some extremely large outliers at small fitted value. This may suggest that the model is still missing some important variables.

Compare the model to two others:

* One using length at birth and gestational age as predictors (main effects only)

* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}
library(modelr)
cv_df =
  crossv_mc(birthweight_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) |>
  mutate(
    mod_my = map(train, \(df) lm(bwt ~ bhead + blength + smoken, data = df)),
    mod_1 = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    mod_2 = map(train, \(df) lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex, data = df)) 
  )|> 
  mutate(
    rmse_my = map2_dbl(mod_my, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_1 = map2_dbl(mod_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_2 = map2_dbl(mod_2, test, \(mod, df) rmse(model = mod, data = df))
  ) 

cv_df |>
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

From the plot, we can see that the performance of our model (using head circumference, length and mother’s race) and the second model (using head circumference, length, sex, and all interactions) are similar, while both are significantly better than the first model (using length at birth and gestational age, main effects only).